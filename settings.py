# ============================================================
#  InsightHub â€” config/settings.py
#  Central configuration for all models, paths & thresholds
#  Stack: HuggingFace (FREE) â€” no OpenAI key needed
# ============================================================

import os
from pathlib import Path
from dotenv import load_dotenv

# Load .env file from project root
load_dotenv()

# â”€â”€ Project Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR        = Path(__file__).resolve().parent.parent
DATA_DIR        = BASE_DIR / "data"
VECTOR_STORE_DIR= BASE_DIR / "vector_store"
UPLOADS_DIR     = DATA_DIR / "uploads"
EXPORTS_DIR     = BASE_DIR / "exports"

# Create directories if they don't exist
for d in [DATA_DIR, VECTOR_STORE_DIR, UPLOADS_DIR, EXPORTS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# â”€â”€ HuggingFace API (Free) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Get your free token at: https://huggingface.co/settings/tokens
HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN", "")

# â”€â”€ Cohere API (Free tier available) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Get free key at: https://dashboard.cohere.com  (1000 calls/month free)
# Leave empty to skip reranking â€” system still works without it
COHERE_API_KEY = os.getenv("COHERE_API_KEY", "")

# â”€â”€ LangSmith (Free for personal use) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sign up at: https://smith.langchain.com
# Enables agent trace logging â€” highly recommended for debugging
LANGCHAIN_API_KEY      = os.getenv("LANGCHAIN_API_KEY", "")
LANGCHAIN_TRACING_V2   = os.getenv("LANGCHAIN_TRACING_V2", "false")
LANGCHAIN_PROJECT       = os.getenv("LANGCHAIN_PROJECT", "insighthub")

# Set LangSmith env vars if key is provided
if LANGCHAIN_API_KEY:
    os.environ["LANGCHAIN_API_KEY"]    = LANGCHAIN_API_KEY
    os.environ["LANGCHAIN_TRACING_V2"] = LANGCHAIN_TRACING_V2
    os.environ["LANGCHAIN_PROJECT"]    = LANGCHAIN_PROJECT

# â”€â”€ Embedding Model (FREE via HuggingFace) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# gte-large: excellent multilingual embeddings, runs locally, no API needed
EMBEDDING_MODEL      = "thenlper/gte-large"
EMBEDDING_DIMENSION  = 1024         # gte-large output dimension
EMBEDDING_DEVICE     = "cpu"        # Change to "cuda" if you have a GPU

# â”€â”€ LLM Model (FREE via HuggingFace Inference API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Mistral-7B-Instruct: best free model for instruction following
# Runs via HuggingFace Inference API â€” no local GPU needed
LLM_MODEL            = "mistralai/Mistral-7B-Instruct-v0.3"
LLM_MAX_NEW_TOKENS   = 1024
LLM_TEMPERATURE      = 0.1          # Low temp = more factual, less creative
LLM_TOP_P            = 0.9

# Alternative FREE models (uncomment to switch):
# LLM_MODEL = "google/flan-t5-large"          # Smaller, faster, less capable
# LLM_MODEL = "HuggingFaceH4/zephyr-7b-beta"  # Good instruction follower
# LLM_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # Lightweight and capable

# â”€â”€ Chunking Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHUNK_SIZE_DOCUMENT = 512           # tokens per chunk for documents
CHUNK_SIZE_WEB      = 800           # tokens per chunk for web pages
CHUNK_SIZE_PAPER    = 512           # tokens per chunk for research papers
CHUNK_OVERLAP       = 50            # overlap between consecutive chunks

# â”€â”€ Retrieval Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RETRIEVAL_TOP_K         = 10        # number of candidates to retrieve
RETRIEVAL_FINAL_K       = 5         # number after reranking
SIMILARITY_THRESHOLD    = 0.65      # minimum cosine similarity score
MMR_DIVERSITY_SCORE     = 0.3       # MMR lambda (0=max diversity, 1=max relevance)
BM25_WEIGHT             = 0.3       # weight for BM25 in hybrid search (0-1)
DENSE_WEIGHT            = 0.7       # weight for dense search in hybrid search

# â”€â”€ Critic Agent Thresholds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITIC_RELEVANCE_THRESHOLD    = 7.0
CRITIC_COHERENCE_THRESHOLD    = 7.0
CRITIC_GROUNDING_THRESHOLD    = 8.0
CRITIC_COMPLETENESS_THRESHOLD = 6.5
CRITIC_MAX_RETRIES            = 2   # max refinement cycles before accepting

# â”€â”€ Source Tab Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SOURCE_TYPES = ["document", "website", "paper"]

DOCUMENT_EXTENSIONS = [".pdf", ".docx", ".txt", ".doc"]
PAPER_ID_PATTERNS   = ["arxiv", "doi", "10."]   # patterns to detect paper IDs

# â”€â”€ Vector Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VECTOR_STORE_TYPE   = "faiss"       # options: "faiss", "chroma"
FAISS_INDEX_NAME    = "insighthub_index"
FAISS_INDEX_PATH    = str(VECTOR_STORE_DIR / FAISS_INDEX_NAME)

# â”€â”€ Web Scraping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REQUEST_TIMEOUT     = 15            # seconds before request times out
MAX_PAGE_CHARS      = 50000         # max characters to keep from a webpage
USE_PLAYWRIGHT      = True          # fallback to Playwright for JS pages

# â”€â”€ Export Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXPORT_DIR          = str(EXPORTS_DIR)
DEFAULT_EXPORT_FORMAT = "markdown"  # options: "markdown", "pdf"

# â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APP_TITLE           = "InsightHub"
APP_SUBTITLE        = "Multi-Agent RAG Â· Research Assistant"
APP_ICON            = "ðŸ”­"

# â”€â”€ Validation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_config():
    """Check critical settings and warn about missing optional ones."""
    warnings = []
    errors   = []

    if not HUGGINGFACE_API_TOKEN:
        errors.append(
            "HUGGINGFACE_API_TOKEN is not set.\n"
            "  â†’ Get your free token at: https://huggingface.co/settings/tokens\n"
            "  â†’ Add to .env file: HUGGINGFACE_API_TOKEN=hf_xxxx"
        )

    if not COHERE_API_KEY:
        warnings.append(
            "COHERE_API_KEY not set â€” reranking disabled.\n"
            "  â†’ System will still work, but retrieval quality may be lower.\n"
            "  â†’ Free key at: https://dashboard.cohere.com"
        )

    if not LANGCHAIN_API_KEY:
        warnings.append(
            "LANGCHAIN_API_KEY not set â€” LangSmith tracing disabled.\n"
            "  â†’ Recommended for debugging agents during development.\n"
            "  â†’ Free at: https://smith.langchain.com"
        )

    if errors:
        print("\n[InsightHub Config] ERRORS â€” Fix before running:")
        for e in errors:
            print(f"  âœ— {e}")

    if warnings:
        print("\n[InsightHub Config] WARNINGS â€” Optional but recommended:")
        for w in warnings:
            print(f"  âš   {w}")

    if not errors:
        print("[InsightHub Config] âœ“ Core configuration valid.")

    return len(errors) == 0


if __name__ == "__main__":
    validate_config()
    print(f"\n  Embedding model : {EMBEDDING_MODEL}")
    print(f"  LLM model       : {LLM_MODEL}")
    print(f"  Vector store    : {VECTOR_STORE_TYPE.upper()} â†’ {FAISS_INDEX_PATH}")
    print(f"  Chunk size      : {CHUNK_SIZE_DOCUMENT} tokens (overlap: {CHUNK_OVERLAP})")
    print(f"  Retrieval top-k : {RETRIEVAL_TOP_K} â†’ reranked to {RETRIEVAL_FINAL_K}")